{"title":"Exploring OpenAI Models","markdown":{"yaml":{"title":"Exploring OpenAI Models"},"headingText":"System prompt","containsRefs":false,"markdown":"\n\n\n\n\nNow that we have verified that we can use the OpenAI API, we can start to use the API to generate text with the GPT-4o-mini and GPT-4o models.\n\nLet's start by generating a response from the GPT-4o-mini model.    \n\n\n\nFirst we need to load the `dotenv` and the `openai` packages.\n\n\nThen we need to load the OpenAI API key from the `.env` file.\n\n\nThen we can create a client to interact with the OpenAI API.\n\nNext we will create a system prompt that will guide the model to explain concepts from music theory in a way that is easy to understand.\n\n:::{.callout-note collapse=true}\n## System prompt\nYou are a primary school music teacher. Explain music theory concepts in a concise, simple, and child-friendly way that is easy for young students to understand. Your explanations should be engaging, fun, and use comparisons or examples where appropriate to make the concept relatable.\nIf a student doesn't ask about a particular topic, introduce an interesting music concept of your own to teach. Remember to keep the language accessible for young learners.\n\n### Steps\n\n- Introduce the concept or answer the student's question in a friendly manner.\n- Use simple, age-appropriate language.\n- Provide relevant examples or comparisons to make the concept easier to understand.\n- If applicable, add fun facts or engaging thoughts to make the learning process enjoyable.\n\n### Output Format\nA short but clear paragraph suitable for a primary school student, between 3-5 friendly sentences.\n\n### Examples\n\n- **Example 1: (student doesn't ask a specific question)**\n- **Concept chosen:** Musical Notes\n- **Explanation:** \"Musical notes are like the letters of the music alphabet! Just like you need letters to make words, you need notes to make songs. Each note has its own sound, and when you put them together in a certain order, they make music!\"\n\n- **Example 2: (student asks about rhythm)**\n- **Question:** What is rhythm in music?\n- **Explanation:** \"Rhythm is like the beat of your favorite song. Imagine you are clapping along to music—that's the rhythm! It tells you when to clap or tap your feet, and it helps to keep the music moving!\"\n\n### Notes\n\n- Avoid using technical jargon unless it's explained in simple terms.\n- Use playful or relatable examples where appropriate (e.g., comparing rhythm to a heartbeat or notes to colors).\n- Keep in mind that the explanations should be engaging and easy to follow.\n:::\n\n## Generate a response\nNow we can generate a response from the GPT-4o-mini model using the system prompt. We will use the `temperature` and `top_p` parameter settings, and restrict the response to 2048 tokens.\n\n\n## Create a function to generate responses\nGoing through the process of generating a response in this manner will soon become tedious, so next we will create a function to generate responses from either the GPT-4o-mini or GPT-4o models, using a specified system prompt, a user message, and `temperature` and `top_p` settings. Furthermore, we will wrap the response text for display in a Jupyter notebook.\n\nThe arguments for the function will be:\n\n- `model`: the OpenAI model to use, either \"gpt-4o-mini\" or \"gpt-4o\"\n- `system_prompt`: the system prompt to use\n- `user_message`: the user message to use\n- `temperature`: the temperature to use, between 0 and 2.0, default 1.0\n- `top_p`: the top_p to use, between 0 and 1.0, default 1.0\n- `max_tokens`: the maximum number of tokens in the response, default 2048\nSome of the arguments have defaults, so they are not required when calling the function.\n\n\nWe can now generate a response from the GPT-4o-mini model using a system prompt and a user message.\n\nWe'll create a simpler system prompt for the next example.\n\n\nWe prompt the model to explain a different concept, e.g. the difference between a major and minor scale.\n\n\n:::{.callout-note}\n## Markdown output\n\nAn issue with the current implementation is that the response given by the model is formatted as Markdown---we hadn't considered how to display Markdown output in a Jupyter notebook, though.\n\n:::\n\n### Improved function for Markdown output\n\n## Exploring the `temperature` and `top_p` parameters\n\n\n\nNow we will explore the effect of changing the `temperature` and `top_p` parameters on the response. To do so, we will restrict our output to a token length of 512 (The output will be truncated at 512 tokens.) \n\n\n### `temperature`: 0, `top-p`: 1.0\n\n\n### `temperature`: 1.5, `top-p`: 1.0\n\n\n### `temperature`: 1.5, `top-p`: 0.8\n\n\n### `temperature`: 1.5, `top-p`: 0.5\n\n\n### `temperature`: 1.8, `top-p`: 1.0\n\n\n### `temperature`: 1.5, `top-p`: 0.5\n\n\n\n:::{.callout-note collapse=true}\n## Discussion of `temperature` and `top_p`\n\nAs the examples above show, the `temperature` and `top_p` parameters can have a significant effect on the response. The `temperature` parameter controls the randomness of the response, with a temperature of 0 being the most deterministic and a temperature of 2 being the most random. The `top_p` parameter controls the diversity of the response.\nIncreasing the temperature above approximately 1.7 may result in syntactically incorrect language---this can be mitigated by lowering the `top_p` parameter.\n\n# Understanding the Interaction Between `top_p` and `temperature` in Text Generation\n\nWhen using language models, the `top_p` and `temperature` parameters play crucial roles in shaping the generated text. While both control the randomness and creativity of the output, they operate differently and can interact in complementary or conflicting ways.\n\n---\n\n### 1. What is `temperature`?\n\nThe `temperature` parameter adjusts the **probability distribution** over the possible next tokens:\n\n- **Lower values (e.g., 0.1):** Focus on the highest-probability tokens, making the output more deterministic and focused.\n- **Higher values (e.g., 1.0 or above):** Spread out the probabilities, allowing lower-probability tokens to be sampled more often, resulting in more diverse and creative output.\n\nMathematically, `temperature` modifies the token probabilities \\( p_i \\) as follows:\n\n$$p_i' = \\frac{p_i^{1/\\text{temperature}}}{\\sum p_i^{1/\\text{temperature}}}$$\n\n- At `temperature = 1.0`: No adjustment, the original probabilities are used.\n- At `temperature < 1.0`: Probabilities are sharpened (more focus on top tokens).\n- At `temperature > 1.0`: Probabilities are flattened (more randomness).\n\n---\n\n### 2. What is `top_p`?\n\nThe `top_p` parameter, also known as nucleus sampling, restricts token selection to those with the highest cumulative probability \\( p \\):\n\n1. Tokens are sorted by their probabilities.\n2. Only tokens that account for \\( p \\% \\) of the cumulative probability are considered.\n   - **Lower values (e.g., 0.1):** Only the most probable tokens are included.\n   - **Higher values (e.g., 0.9):** A broader set of tokens is included, allowing for more diverse outputs.\n\nUnlike `temperature`, `top_p` dynamically adapts to the shape of the probability distribution.\n\n\n### 3. How Do `temperature` and `top_p` Interact?\n\n#### a. **Low `temperature` + Low `top_p`**\n- Behavior: Highly deterministic.\n- Use Case: Tasks requiring precise and factual responses (e.g., technical documentation, Q&A).\n- Interaction: \n  - **Low `temperature`** sharply focuses the probability distribution, and **low `top_p`** further restricts token choices.\n  - Result: Very narrow and predictable outputs.\n\n#### b. **Low `temperature` + High `top_p`**\n- Behavior: Slightly creative but still constrained.\n- Use Case: Formal content generation with slight variability.\n- Interaction:\n  - **Low `temperature`** ensures focused probabilities, but **high `top_p`** allows more token options.\n  - Result: Outputs are coherent with minimal creativity.\n\n#### c. **High `temperature` + Low `top_p`**\n- Behavior: Controlled randomness.\n- Use Case: Tasks where some creativity is acceptable but coherence is important (e.g., storytelling with a clear structure).\n- Interaction:\n  - **High `temperature`** flattens the probabilities, introducing more randomness, but **low `top_p`** limits the selection to the most probable tokens.\n  - Result: Outputs are creative but still coherent.\n\n#### d. **High `temperature` + High `top_p`**\n- Behavior: Highly creative and diverse.\n- Use Case: Tasks requiring out-of-the-box ideas (e.g., brainstorming, poetry).\n- Interaction:\n  - **High `temperature`** increases randomness, and **high `top_p`** allows even lower-probability tokens to be included.\n  - Result: Outputs can be very diverse, sometimes sacrificing coherence.\n\n---\n\n### 4. Practical Guidelines\n\n#### Balancing Creativity and Coherence\n- Start with default values (`temperature = 1.0`, `top_p = 1.0`).\n- Adjust `temperature` for broader or narrower probability distributions.\n- Adjust `top_p` to fine-tune the token selection process.\n\n#### Common Configurations\n| **Scenario**                  | **Temperature** | **Top_p** | **Description**                                     |\n|-------------------------------|-----------------|-----------|---------------------------------------------------|\n| Precise and Deterministic     | 0.1             | 0.3       | Outputs are highly focused and factual.          |\n| Balanced Creativity           | 0.7             | 0.8–0.9   | Outputs are coherent with some diversity.        |\n| Controlled Randomness         | 1.0             | 0.5–0.7   | Allows for creativity while maintaining structure.|\n| Highly Creative               | 1.2 or higher   | 0.9–1.0   | Outputs are diverse and may deviate from structure.|\n\n---\n\n### 5. Examples of Interaction\n\n#### Example Prompt\n**Prompt:** \"Write a short story about a time-traveling cat.\"\n\n1. **Low `temperature`, low `top_p`:**\n   - Output: \"The cat found a time machine and traveled to ancient Egypt.\"\n   - Description: Simple, predictable story.\n\n2. **High `temperature`, low `top_p`:**\n   - Output: \"The cat stumbled upon a time vortex and arrived in a land ruled by cheese-loving robots.\"\n   - Description: Random but slightly constrained.\n\n3. **High `temperature`, high `top_p`:**\n   - Output: \"The cat discovered a mystical clock, its paws adjusting gears to jump into dimensions where history danced with dreams.\"\n   - Description: Wildly creative and poetic.\n\n---\n\n### 6. Conclusion\n\nThe `temperature` and `top_p` parameters are powerful tools for controlling the style and behavior of text generation. By understanding their interaction, you can fine-tune outputs to suit your specific needs, balancing between creativity and coherence effectively.\n\nExperiment with these parameters to find the sweet spot for your particular application.\n:::\n\n## Generating multiple responses\n\nWe can also generate multiple responses from the model by setting the `n` parameter to a value greater than 1. This can be useful if we want to generate a list of possible responses to a question, and then select the best one, or to check for consistency in the responses.\n\n\n\nNow we can choose one of the responses.\n\nWe can also loop through the responses and print them all.\n\n","srcMarkdownNoYaml":"\n\n\n\n\nNow that we have verified that we can use the OpenAI API, we can start to use the API to generate text with the GPT-4o-mini and GPT-4o models.\n\nLet's start by generating a response from the GPT-4o-mini model.    \n\n\n\nFirst we need to load the `dotenv` and the `openai` packages.\n\n\nThen we need to load the OpenAI API key from the `.env` file.\n\n\nThen we can create a client to interact with the OpenAI API.\n\n## System prompt\nNext we will create a system prompt that will guide the model to explain concepts from music theory in a way that is easy to understand.\n\n:::{.callout-note collapse=true}\n## System prompt\nYou are a primary school music teacher. Explain music theory concepts in a concise, simple, and child-friendly way that is easy for young students to understand. Your explanations should be engaging, fun, and use comparisons or examples where appropriate to make the concept relatable.\nIf a student doesn't ask about a particular topic, introduce an interesting music concept of your own to teach. Remember to keep the language accessible for young learners.\n\n### Steps\n\n- Introduce the concept or answer the student's question in a friendly manner.\n- Use simple, age-appropriate language.\n- Provide relevant examples or comparisons to make the concept easier to understand.\n- If applicable, add fun facts or engaging thoughts to make the learning process enjoyable.\n\n### Output Format\nA short but clear paragraph suitable for a primary school student, between 3-5 friendly sentences.\n\n### Examples\n\n- **Example 1: (student doesn't ask a specific question)**\n- **Concept chosen:** Musical Notes\n- **Explanation:** \"Musical notes are like the letters of the music alphabet! Just like you need letters to make words, you need notes to make songs. Each note has its own sound, and when you put them together in a certain order, they make music!\"\n\n- **Example 2: (student asks about rhythm)**\n- **Question:** What is rhythm in music?\n- **Explanation:** \"Rhythm is like the beat of your favorite song. Imagine you are clapping along to music—that's the rhythm! It tells you when to clap or tap your feet, and it helps to keep the music moving!\"\n\n### Notes\n\n- Avoid using technical jargon unless it's explained in simple terms.\n- Use playful or relatable examples where appropriate (e.g., comparing rhythm to a heartbeat or notes to colors).\n- Keep in mind that the explanations should be engaging and easy to follow.\n:::\n\n## Generate a response\nNow we can generate a response from the GPT-4o-mini model using the system prompt. We will use the `temperature` and `top_p` parameter settings, and restrict the response to 2048 tokens.\n\n\n## Create a function to generate responses\nGoing through the process of generating a response in this manner will soon become tedious, so next we will create a function to generate responses from either the GPT-4o-mini or GPT-4o models, using a specified system prompt, a user message, and `temperature` and `top_p` settings. Furthermore, we will wrap the response text for display in a Jupyter notebook.\n\nThe arguments for the function will be:\n\n- `model`: the OpenAI model to use, either \"gpt-4o-mini\" or \"gpt-4o\"\n- `system_prompt`: the system prompt to use\n- `user_message`: the user message to use\n- `temperature`: the temperature to use, between 0 and 2.0, default 1.0\n- `top_p`: the top_p to use, between 0 and 1.0, default 1.0\n- `max_tokens`: the maximum number of tokens in the response, default 2048\nSome of the arguments have defaults, so they are not required when calling the function.\n\n\nWe can now generate a response from the GPT-4o-mini model using a system prompt and a user message.\n\nWe'll create a simpler system prompt for the next example.\n\n\nWe prompt the model to explain a different concept, e.g. the difference between a major and minor scale.\n\n\n:::{.callout-note}\n## Markdown output\n\nAn issue with the current implementation is that the response given by the model is formatted as Markdown---we hadn't considered how to display Markdown output in a Jupyter notebook, though.\n\n:::\n\n### Improved function for Markdown output\n\n## Exploring the `temperature` and `top_p` parameters\n\n\n\nNow we will explore the effect of changing the `temperature` and `top_p` parameters on the response. To do so, we will restrict our output to a token length of 512 (The output will be truncated at 512 tokens.) \n\n\n### `temperature`: 0, `top-p`: 1.0\n\n\n### `temperature`: 1.5, `top-p`: 1.0\n\n\n### `temperature`: 1.5, `top-p`: 0.8\n\n\n### `temperature`: 1.5, `top-p`: 0.5\n\n\n### `temperature`: 1.8, `top-p`: 1.0\n\n\n### `temperature`: 1.5, `top-p`: 0.5\n\n\n\n:::{.callout-note collapse=true}\n## Discussion of `temperature` and `top_p`\n\nAs the examples above show, the `temperature` and `top_p` parameters can have a significant effect on the response. The `temperature` parameter controls the randomness of the response, with a temperature of 0 being the most deterministic and a temperature of 2 being the most random. The `top_p` parameter controls the diversity of the response.\nIncreasing the temperature above approximately 1.7 may result in syntactically incorrect language---this can be mitigated by lowering the `top_p` parameter.\n\n# Understanding the Interaction Between `top_p` and `temperature` in Text Generation\n\nWhen using language models, the `top_p` and `temperature` parameters play crucial roles in shaping the generated text. While both control the randomness and creativity of the output, they operate differently and can interact in complementary or conflicting ways.\n\n---\n\n### 1. What is `temperature`?\n\nThe `temperature` parameter adjusts the **probability distribution** over the possible next tokens:\n\n- **Lower values (e.g., 0.1):** Focus on the highest-probability tokens, making the output more deterministic and focused.\n- **Higher values (e.g., 1.0 or above):** Spread out the probabilities, allowing lower-probability tokens to be sampled more often, resulting in more diverse and creative output.\n\nMathematically, `temperature` modifies the token probabilities \\( p_i \\) as follows:\n\n$$p_i' = \\frac{p_i^{1/\\text{temperature}}}{\\sum p_i^{1/\\text{temperature}}}$$\n\n- At `temperature = 1.0`: No adjustment, the original probabilities are used.\n- At `temperature < 1.0`: Probabilities are sharpened (more focus on top tokens).\n- At `temperature > 1.0`: Probabilities are flattened (more randomness).\n\n---\n\n### 2. What is `top_p`?\n\nThe `top_p` parameter, also known as nucleus sampling, restricts token selection to those with the highest cumulative probability \\( p \\):\n\n1. Tokens are sorted by their probabilities.\n2. Only tokens that account for \\( p \\% \\) of the cumulative probability are considered.\n   - **Lower values (e.g., 0.1):** Only the most probable tokens are included.\n   - **Higher values (e.g., 0.9):** A broader set of tokens is included, allowing for more diverse outputs.\n\nUnlike `temperature`, `top_p` dynamically adapts to the shape of the probability distribution.\n\n\n### 3. How Do `temperature` and `top_p` Interact?\n\n#### a. **Low `temperature` + Low `top_p`**\n- Behavior: Highly deterministic.\n- Use Case: Tasks requiring precise and factual responses (e.g., technical documentation, Q&A).\n- Interaction: \n  - **Low `temperature`** sharply focuses the probability distribution, and **low `top_p`** further restricts token choices.\n  - Result: Very narrow and predictable outputs.\n\n#### b. **Low `temperature` + High `top_p`**\n- Behavior: Slightly creative but still constrained.\n- Use Case: Formal content generation with slight variability.\n- Interaction:\n  - **Low `temperature`** ensures focused probabilities, but **high `top_p`** allows more token options.\n  - Result: Outputs are coherent with minimal creativity.\n\n#### c. **High `temperature` + Low `top_p`**\n- Behavior: Controlled randomness.\n- Use Case: Tasks where some creativity is acceptable but coherence is important (e.g., storytelling with a clear structure).\n- Interaction:\n  - **High `temperature`** flattens the probabilities, introducing more randomness, but **low `top_p`** limits the selection to the most probable tokens.\n  - Result: Outputs are creative but still coherent.\n\n#### d. **High `temperature` + High `top_p`**\n- Behavior: Highly creative and diverse.\n- Use Case: Tasks requiring out-of-the-box ideas (e.g., brainstorming, poetry).\n- Interaction:\n  - **High `temperature`** increases randomness, and **high `top_p`** allows even lower-probability tokens to be included.\n  - Result: Outputs can be very diverse, sometimes sacrificing coherence.\n\n---\n\n### 4. Practical Guidelines\n\n#### Balancing Creativity and Coherence\n- Start with default values (`temperature = 1.0`, `top_p = 1.0`).\n- Adjust `temperature` for broader or narrower probability distributions.\n- Adjust `top_p` to fine-tune the token selection process.\n\n#### Common Configurations\n| **Scenario**                  | **Temperature** | **Top_p** | **Description**                                     |\n|-------------------------------|-----------------|-----------|---------------------------------------------------|\n| Precise and Deterministic     | 0.1             | 0.3       | Outputs are highly focused and factual.          |\n| Balanced Creativity           | 0.7             | 0.8–0.9   | Outputs are coherent with some diversity.        |\n| Controlled Randomness         | 1.0             | 0.5–0.7   | Allows for creativity while maintaining structure.|\n| Highly Creative               | 1.2 or higher   | 0.9–1.0   | Outputs are diverse and may deviate from structure.|\n\n---\n\n### 5. Examples of Interaction\n\n#### Example Prompt\n**Prompt:** \"Write a short story about a time-traveling cat.\"\n\n1. **Low `temperature`, low `top_p`:**\n   - Output: \"The cat found a time machine and traveled to ancient Egypt.\"\n   - Description: Simple, predictable story.\n\n2. **High `temperature`, low `top_p`:**\n   - Output: \"The cat stumbled upon a time vortex and arrived in a land ruled by cheese-loving robots.\"\n   - Description: Random but slightly constrained.\n\n3. **High `temperature`, high `top_p`:**\n   - Output: \"The cat discovered a mystical clock, its paws adjusting gears to jump into dimensions where history danced with dreams.\"\n   - Description: Wildly creative and poetic.\n\n---\n\n### 6. Conclusion\n\nThe `temperature` and `top_p` parameters are powerful tools for controlling the style and behavior of text generation. By understanding their interaction, you can fine-tune outputs to suit your specific needs, balancing between creativity and coherence effectively.\n\nExperiment with these parameters to find the sweet spot for your particular application.\n:::\n\n## Generating multiple responses\n\nWe can also generate multiple responses from the model by setting the `n` parameter to a value greater than 1. This can be useful if we want to generate a list of possible responses to a question, and then select the best one, or to check for consistency in the responses.\n\n\n\nNow we can choose one of the responses.\n\nWe can also loop through the responses and print them all.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"execute-dir":"project","engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":true,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"brand":{"brand":{"color":{"palette":{"dark-grey":"#222222","blue":"#ddeaf1"},"background":"blue","foreground":"dark-grey","primary":"black"},"typography":{"fonts":[{"family":"Jura","source":"google"}],"base":"Jura","headings":"Jura"}},"data":{"color":{"palette":{"dark-grey":"#222222","blue":"#ddeaf1"},"background":"blue","foreground":"dark-grey","primary":"black"},"typography":{"fonts":[{"family":"Jura","source":"google"}],"base":"Jura","headings":"Jura"}},"brandDir":"/Users/andrew/GitHub/sites/ai-education-assignments-2025","projectDir":"/Users/andrew/GitHub/sites/ai-education-assignments-2025","processedData":{"color":{"dark-grey":"#222222","blue":"#ddeaf1","background":"#ddeaf1","foreground":"#222222","primary":"black"},"typography":{"base":"Jura","headings":"Jura"},"logo":{"images":{}}}}},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["timer","custom-callout"],"toc":true,"output-file":"exploring-openai-models.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","page-footer":{"right":[{"icon":"github","href":"https://github.com/virtuelleakademie/ai-education-assignments-2025"}]},"comments":{"hypothesis":{"theme":"clean"}},"editor":{"render-on-save":true},"revealjs-plugins":["attribution"],"custom-callout":{"individual":{"title":"Individual Work","icon-symbol":"💻","color":"steelblue","collapse":false},"pair":{"title":"Pair Work","icon-symbol":"👥","color":"forestgreen","collapse":false},"group":{"title":"Group Work","icon-symbol":"👥","color":"purple","collapse":false},"screens-down":{"title":"Screens Down","icon-symbol":"🔽","color":"orange","collapse":false},"screens-up":{"title":"Screens Up","icon-symbol":"🔼","color":"green","collapse":false},"try":{"title":"Try It","icon-symbol":"🔄","color":"blue","collapse":false},"reflect":{"title":"Reflection","icon-symbol":"💭","color":"purple","collapse":false},"feedback":{"title":"Feedback","icon-symbol":"📝","color":"teal","collapse":true},"caution":{"title":"Watch Out","icon-symbol":"⚠️","color":"orange","collapse":true},"pro-tip":{"title":"Pro Tip","icon-symbol":"💡","color":"gold","collapse":true},"prompt-example":{"title":"Example Prompt","icon-symbol":"👩🏼‍🔧","color":"steelblue","collapse":false},"output-example":{"title":"Output Example","icon-symbol":"📄","color":"purple","collapse":true},"prompt-template":{"title":"Prompt Template","icon-symbol":"📋","color":"indigo","collapse":true},"testing":{"title":"Test Your Prompt","icon-symbol":"🧪","color":"green","collapse":false},"export":{"title":"Export","icon-symbol":"📤","color":"brown","collapse":true},"setup":{"title":"Setup","icon-symbol":"⚙️","color":"gray","collapse":true},"timing":{"title":"Time Check","icon-symbol":"⏱️","color":"red","collapse":false},"break":{"title":"Break Time","icon-symbol":"☕","color":"yellow","collapse":false},"checkpoint":{"title":"Checkpoint","icon-symbol":"🎯","color":"purple","collapse":false},"schedule":{"title":"Schedule","icon-symbol":"⏰︎","color":"steelblue","collapse":false}},"theme":{"light":"simplex"},"anchor-sections":true,"smooth-scroll":true,"linkcolor":"rebeccapurple","title":"Exploring OpenAI Models"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}